# Talk Shop

**Talk Shop** is a voice-first shopping assistant that helps users discover products through natural conversation.
Instead of filters, forms, and endless scrolling, users simply talk about what they want â€” and what they like or dislike â€” while the app learns their preferences over time.

> *You donâ€™t search. You talk shop.*

---

## âœ¨ What Talk Shop Does

* Lets users **shop by speaking naturally**
* Shows **one product at a time**, anchored by images
* Learns preferences implicitly from likes, dislikes, questions, and skips
* Builds a **long-term preference profile** that works across categories
* Improves recommendations without forcing users to configure settings

---

## ðŸ—£ï¸ Core Interaction Model

### 1. Start with intent

Users begin with a natural prompt:

> â€œI want to buy shoes that match the color scheme of the Brazilian soccer team.â€

Talk Shop interprets this and visually confirms it (e.g. green/yellow/blue color cues), then immediately shows a product.

---

### 2. React to a product

Users mostly respond by commenting on what they see:

* â€œThe design is too flashyâ€
* â€œI like the brandâ€
* â€œWhat material are these made of?â€
* â€œNextâ€

These responses are treated as **signals**, not commands.

---

### 3. Implicit preference learning

Talk Shop infers preferences based on context:

* Saying *â€œtoo flashyâ€* â†’ preference for subtler designs
* Asking about materials â†’ interest in composition
* Hearing *â€œPU leatherâ€* followed by *â€œnextâ€* â†’ likely avoidance of faux leather
* Saying *â€œI like the brandâ€* â†’ brand affinity

Preferences are:

* Weighted (soft vs hard)
* Scoped (session-only vs long-term)
* Reversible (users can correct the system)

---

### 4. Iterate

Each interaction refines the next recommendation.
The assistant may ask a clarification **only when necessary**.

---

## ðŸ§  Preference Profile

Talk Shop maintains a continuously evolving profile built from user behavior.

### Preference types

* **Hard constraints**
  (e.g. â€œunder $150â€, â€œno faux leatherâ€)
* **Soft preferences**
  (e.g. subtle design, preferred brands, natural materials)
* **Visual/style signals**
  (learned from liked vs skipped products)

### Cross-category learning

Preferences can transfer across categories when appropriate.

Example:

* User avoids synthetic leather when shopping for shoes
* Later, when shopping for sweaters, Talk Shop prioritizes natural fabrics

This happens **silently and conservatively**, with low confidence until reinforced.

---

## ðŸ–¼ï¸ UI Principles

* **One product at a time** (no grids)
* Large image as the primary focus
* Clear attribute chips (brand, material, style, color, price)
* Voice-first, touch-optional
* Minimal confirmations â€” behavior proves understanding

---

## ðŸ” Transparency & Trust

To avoid â€œblack boxâ€ behavior:

* Visual cues reflect what the system is learning
* Preferences can be viewed and edited in a dedicated drawer
* Explanations are provided *only when asked*

---

## ðŸš« What Talk Shop Is Not

* Not a voice command interface
* Not a chatbot that asks constant follow-ups
* Not a static recommender based only on past purchases

Talk Shop behaves like a **good salesperson**:

* Attentive
* Quietly adaptive
* Willing to be corrected

---

## ðŸ§ª MVP Scope (Suggested)

* Single category (e.g. shoes)
* Fixed attribute schema
* Voice input + image output
* Like / dislike / next / questions
* Session-based preference learning

---

## ðŸ“Œ Product Philosophy

> People are good at reacting, not specifying.

Talk Shop is designed around reactions â€” what users notice, like, dislike, or skip â€” and turns those reactions into better recommendations over time.
